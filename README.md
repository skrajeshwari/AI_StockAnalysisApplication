# Stock insights application

This is a repository for the Stock Prices Insights project. 

Project Flow: Each query a user makes will follow one of three paths, depending on the nature of the request. The flow is designed to simulate real-world interaction with LLM-driven systems but is tailored to demonstrate how the technology works with structured data.

Path 1: For Queries Needing Data Access and Insights These queries require retrieving data from the database, running additional analyses, and summarizing insights.
Steps: LLM generates an SQL query → Retrieves data from the database → Python code for analysis → Insight generation → Suggestions for next steps.
This demonstrates how a LLM dynamically creates SQL queries and generates meaningful outputs from structured data.

Path 2: For Exploratory or Informational Questions These queries are more exploratory and don’t require accessing data, like asking about the available dataset or metadata.
Steps: LLM responds directly based on its understanding → It offers suggestions for further exploration.
This demonstrates how to leverage LLMs for general conversation-style queries and data exploration without needing heavy data processing.

Path 3: For Out-of-Scope or Irrelevant Questions Queries that are outside the scope of the system (e.g., "What is the weather today?").
Steps: LLM generates a polite response indicating the question is out of scope → Suggests relevant questions within the system’s scope.

Technical Stack:

LLM: Pre-trained large language models (e.g., GPT-4) for query interpretation and code generation.
SQL Database: Storing structured data (e.g., Nifty 50 financial data) that can be queried based on user requests.
Python: Used for data manipulation and analysis (with libraries like pandas), allowing the system to generate insights from the raw data.
APIs/Integrations: Used to create an interface between the LLM, SQL database, and Python execution environment.

In this project, we are:

1. Loading the data from pickle files(constituent_stock_prices.pkl, constituent_stock_fundamentals.pkl).
2. Storing the data in a SQLite database (Tables: stock_prices, income_statement, balancesheet_statement, cashflow_statement).
3. Using LLMs to create chains for querying insights from this database.

Each table stores financial data related to companies, identified by their symbols in capital letters. The stock_prices table tracks daily stock price information, while the other three tables provide detailed financial statements, including income, balance sheet, and cash flow data for various companies.

The chatbot application is built with Chainlit and allows users to ask questions within context and get relevant responses generated by the LLM.

## Steps to Follow

1. Add your OpenAI key to your repository Secrets. 
   
   Go to Settings -> Secrets and Variables -> Codespaces -> New repository secret -> Give Name(eg. OPENAI_KEY) and paste Secret Value

2. Start a Codespace by going to `Code` dropdown > Select `Codespaces` tab > Click on `Create codespace on main`

3. Create and activate a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate
   ```

4. Install requirement:
   ```
   pip install -r requirements/requirements.txt
   ```

5. Read data from pickle files and create a database:
   ```
   python main.py
   ```

6. Start application:
   ```
   chainlit run app.py
   ```

7. Once the application is running, access it in the browser

8. Stop the application by pressing `Ctrl + C`

9. Delete the Codespace by going to `Code` dropdown > Select `Codespaces` tab > Click on 3 dots (...) showing against your codespace and select `Delete`
